{"pdf_url":"https://arxiv.org/pdf/1810.04805.pdf", "chat_history":[], "question": "What is the main difference between BERT and previous language representation models?", "groundtruth": "BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.", "context": "Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.", "config":{"EMBEDDING_MODEL_DEPLOYMENT_NAME":"text-embedding-ada-002","CHAT_MODEL_DEPLOYMENT_NAME":"aoai","PROMPT_TOKEN_LIMIT":2000,"MAX_COMPLETION_TOKENS":256,"CHUNK_SIZE":1024,"CHUNK_OVERLAP":64}}
{"pdf_url":"https://arxiv.org/pdf/1810.04805.pdf", "chat_history":[], "question": "What is the size of the vocabulary used by BERT?", "groundtruth": "30,000", "context": "We use WordPiece embeddings (Wu et al., 2016) with a 30,000 token vocabulary.", "config":{"EMBEDDING_MODEL_DEPLOYMENT_NAME":"text-embedding-ada-002","CHAT_MODEL_DEPLOYMENT_NAME":"aoai","PROMPT_TOKEN_LIMIT":2000,"MAX_COMPLETION_TOKENS":256,"CHUNK_SIZE":1024,"CHUNK_OVERLAP":64}}
{"pdf_url":"https://grs.pku.edu.cn/docs/2018-03/20180301083100898652.pdf", "chat_history":[], "question": "论文写作中论文引言有什么注意事项？", "groundtruth":"", "context":"", "config":{"EMBEDDING_MODEL_DEPLOYMENT_NAME":"text-embedding-ada-002","CHAT_MODEL_DEPLOYMENT_NAME":"aoai","PROMPT_TOKEN_LIMIT":2000,"MAX_COMPLETION_TOKENS":256,"CHUNK_SIZE":1024,"CHUNK_OVERLAP":64}}