name: pf_in_aml_pipeline_workflow.yml

on:
  workflow_call:
    inputs:
      env_name:
        type: string
        description: "Execution Environment"
        required: true
        default: "dev"
      flow_type:
        type: string
        description: "The flow use-case to execute"
        required: true
        default: "web_classification"
      deployment_type:
        type: string
        description: "Determine type of deployment - aml, aks, docker, webapp"
        required: true
      compute_target:
        type: string
        description: "Azure ML studio Compute target "
        required: true
    secrets:
      azure_credentials:
        description: "service principal authentication to Azure"
        required: true
jobs:
  flow-experiment-and_evaluation:
    name: prompt flow experiment and evaluation job
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Actions
        uses: actions/checkout@v4

      - name: Azure login
        uses: azure/login@v1
        with:
          creds: ${{ secrets.azure_credentials }}

      - name: Configure Azure ML Agent
        uses: ./.github/actions/configure_azureml_agent

      - name: load the current Azure subscription details
        id: subscription_details
        shell: bash
        run: |
          export subscriptionId=$(az account show --query id -o tsv)
          echo "SUBSCRIPTION_ID=$subscriptionId" >> $GITHUB_OUTPUT

        #=====================================
        # Registers experiment dataset in Azure ML as Data Asset
        # Reads appropriate field values from data_config.json based on environment and data purpose
        #=====================================      
      - name: Register experiment data asset
        uses: ./.github/actions/execute_script
        with:
            step_name: "Register experiment data asset"
            script_parameter: |
                python llmops.common.promptflow_in_aml_pipeline.py \
                --subscription_id ${{ steps.subscription_details.outputs.SUBSCRIPTION_ID }} \
                --env_name ${{ inputs.env_name }} \
                --data_purpose=training_data \
                --compute_target= ${{ inputs.compute_target }} 
                --flow_to_execute ${{ inputs.flow_type }} 
                --build_id=build1
