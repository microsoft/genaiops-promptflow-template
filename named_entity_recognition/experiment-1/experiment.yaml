name: named_entity_recognition
flow: flows/standard

datasets:
- name: named_entity_recognition_train
  source: data/data.jsonl
  description: "This dataset is for prompt experiments."
  mappings:
    text: "${data.text}"
    entity_type: "${data.entity_type}"

evaluators:
- name: matcher
  flow: flows/evaluation
  datasets:
  - name: named_entity_recognition_test
    reference: named_entity_recognition_train
    source: data/eval_data.jsonl
    description: "This dataset is for evaluating flows."
    mappings:
      ground_truth: "${data.results}"
      entities: "${run.outputs.entities}"


# - name: eval
#   datasets: 
#   - name: named_entity_recognition_train
#     mappings:
#       ground_truth: "${data.results}"
#       entities: "${run.outputs.entities}"

runtime: "runtime_name"

# datasets 
# All defined datasets are run for the standard flow, except those with "reference" parameter - those ones are exclusive for the evaluators
# "source" parameter is either azureml:<dataset_name>:<dataset_version> or path to file
# "source" parameter pauses questions when it is a path to file:
#  1. Will a standard/evaluation run always register a new version of the dataset? 
#  2. Or should we just use the local data path when creating the promptflow `Run` (AzureML might still register a dataset under the hood)?
#  3. Should the data asset registration be a pre-step in the pipeline, in that case we always get the "latest" version of the data (this is current implementation)?
# Evaluators that point to a dataset WITHOUT a reference know that the standard run used the same dataset, those that point to a dataset WITH reference know that the standard run used the reference dataset.
